{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import exodata\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import exodata.astroquantities as aq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):  \n",
    "    \"\"\"pass in list, returns string\"\"\"\n",
    "    str1 = \",\" \n",
    "    return (str1.join(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findOtherName(check, primary, alts):\n",
    "    \"\"\"extract other names & ID number and arrange them in formatting that matches current standard stellar naming conventions\n",
    "    ex: stars['altnames'] = ['11 com b', 'Gliese 234', 'HD137'] -> stars['HD'] = '137'\"\"\"\n",
    "    name = \"\"\n",
    "    if primary.startswith(check):\n",
    "        name = primary[len(check) + 1:]\n",
    "    if name == \"\" and alts == \"\":\n",
    "        return np.nan\n",
    "    for i in range(len(alts)):\n",
    "        if alts[i].startswith(check):\n",
    "            name = alts[i][len(check) + 1:]\n",
    "    if name == \"\":\n",
    "        return np.nan\n",
    "    while re.search(\"\\D\", name):\n",
    "        name = name[:-1]\n",
    "    return float(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findGLName(primary, alts):\n",
    "    \"\"\"extract Gliese name & ID number and arrange them in formatting that matches current standard stellar naming conventions\n",
    "    ex: stars['altnames'] = ['11 com b', 'Gliese 234', 'HD137'] -> stars['GL'] = 'GL 234'\"\"\"\n",
    "    prefixes = [\"GL \", \"Gliese \", \"NN \", \"WO \", \"GJ \"]\n",
    "    name = \"\"\n",
    "    for i in range(len(prefixes)):\n",
    "        if primary.startswith(prefixes[i]):\n",
    "            name = primary\n",
    "            break\n",
    "    if name == \"\" and alts != \"\":\n",
    "        for i in range(len(alts)):\n",
    "            for j in range(len(prefixes)):\n",
    "                if alts[i].startswith(prefixes[j]):\n",
    "                    name = alts[i]\n",
    "                    break\n",
    "            if name != \"\":\n",
    "                break\n",
    "    if name == \"\":\n",
    "        return np.nan\n",
    "    if name.startswith(\"GL\"):\n",
    "        name = \"Gl\" + name[2:]\n",
    "    elif name.startswith(\"WO\"):\n",
    "        name = \"Wo\" + name[2:]\n",
    "    elif name.startswith(\"Gliese\"):\n",
    "        name = \"GJ\" + name[6:]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListOfNames(names):\n",
    "    \"\"\"Pass in a string from dataframe that has names surrounded by parentheses you'd like to extract the names from\n",
    "    ex: stars['name'] = Star('11 com b') -> stars['name'] = '11 com b'\"\"\"\n",
    "    temp = \"\"\n",
    "    while len(names) > 2:\n",
    "        start = names.find('(') + 2\n",
    "        end = names.find(')') - 1\n",
    "        temp += names[start:end] if len(temp) == 0 else \", \" + names[start:end]\n",
    "        names = names[end + 2 :]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(x):\n",
    "    \"\"\"pass in a value you'd like to check to see if it is not a number, not to be confused with .isna() which returns checks if null\"\"\"\n",
    "    try:\n",
    "        float(x)\n",
    "    except:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catagorize(columns, name):\n",
    "    \"\"\"Given a list of columns and a name, adds all columns into single column under new name\"\"\"\n",
    "    data[name] = data[columns[0]]\n",
    "    for item in columns[1:]:\n",
    "        data[name] = data[name] + data[item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: IllegalSecondWarning: 'second' was found  to be '60', which is not in range [0,60). Treating as 0 sec, +1 min [astropy.coordinates.angle_utilities]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rejected duplicate satellite: \n",
      "\t\t\t\t in Jupiter\n",
      "rejected duplicate satellite: \n",
      "\t\t\t\t in Jupiter\n",
      "rejected duplicate satellite: \n",
      "\t\t\t\t in Jupiter\n",
      "rejected duplicate satellite: \n",
      "\t\t\t\t in Saturn\n",
      "rejected duplicate satellite: \n",
      "\t\t\t\t in Saturn\n",
      "rejected duplicate satellite: \n",
      "\t\t\t\t in Saturn\n",
      "rejected duplicate satellite: \n",
      "\t\t\t\t in Saturn\n",
      "rejected duplicate satellite: \n",
      "\t\t\t\t in Saturn\n",
      "rejected duplicate satellite: \n",
      "\t\t\t\t in Uranus\n",
      "rejected duplicate satellite: \n",
      "\t\t\t\t in Uranus\n",
      "rejected duplicate satellite: \n",
      "\t\t\t\t in Uranus\n",
      "rejected duplicate satellite: \n",
      "\t\t\t\t in Uranus\n"
     ]
    }
   ],
   "source": [
    "# load the most current data from the Open Exoplanet Catalouge from db url for most up to date & add csv with more stars that do not have planets \n",
    "exocat = exodata.load_db_from_url('https://github.com/OpenExoplanetCatalogue/oec_gzip/raw/master/systems.xml.gz')\n",
    "star_csv = pd.read_csv('data/hygdata_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign names to tree branches we'll be using \n",
    "planets = exocat.planets\n",
    "stars = exocat.stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove every bit of info from the xml and transfer it to a dataframe, first we'll be making each of the branches into it's own dictionary \n",
    "star_dict = {}\n",
    "\n",
    "#then we iterate over each element in the branch and fill the dictionaries with the information they contain, data is stored in this tree\n",
    "#in both callable methods, and within a dictionary within one of those methods, which makes it kind of a mess to extract \n",
    "#also the module is written to raise errors instead of returning nothing, so everything has to be wrapped in a try statement with nan being fill if not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\learn-env\\lib\\site-packages\\quantities\\quantity.py:424: RuntimeWarning: invalid value encountered in greater\n",
      "  return self.magnitude > other\n",
      "C:\\ProgramData\\anaconda3\\envs\\learn-env\\lib\\site-packages\\quantities\\quantity.py:391: RuntimeWarning: invalid value encountered in less\n",
      "  return self.magnitude < other\n"
     ]
    }
   ],
   "source": [
    "# don't seem to be able to parse through the xml without it being in try/excepts \n",
    "i = 0 \n",
    "while i < 3505:\n",
    "    star_dict[i] = {}\n",
    "    try:\n",
    "        star_dict[i]['spectral_type'] = stars[i].params['spectraltype']\n",
    "    except: \n",
    "        star_dict[i]['spectral_type'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['temp'] = stars[i].params['temperature']\n",
    "    except:\n",
    "        star_dict[i]['temp'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['metallicity'] = stars[i].params['metallicity']\n",
    "    except:\n",
    "        star_dict[i]['metallicity'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['altnamesstr'] = listToString(stars[i].params['altnames'])\n",
    "    except:\n",
    "        star_dict[i]['altnames'] = ''\n",
    "    try:\n",
    "        star_dict[i]['altnames'] = stars[i].params['altnames']\n",
    "    except:\n",
    "        star_dict[i]['altnames'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['mass'] = stars[i].params['mass']\n",
    "    except:\n",
    "        star_dict[i]['mass'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['magUltraviolet'] = stars[i].params['magU']\n",
    "    except:\n",
    "        star_dict[i]['magUltraviolet'] = np.nan\n",
    "    try: \n",
    "        star_dict[i]['magBlue'] = stars[i].params['magB']\n",
    "    except:\n",
    "        star_dict[i]['magBlue'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['magH_nearinfared'] = stars[i].params['magH']\n",
    "    except:\n",
    "        star_dict[i]['magH_nearinfared'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['magInfared'] = stars[i].params['magI']\n",
    "    except: \n",
    "        star_dict[i]['magInfared'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['magJ_nearinfared'] = stars[i].params['magJ']\n",
    "    except: \n",
    "        star_dict[i]['magJ_nearinfared'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['magK_nearinfared'] = stars[i].params['magK']\n",
    "    except: \n",
    "        star_dict[i]['magK_nearinfared'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['magVisual'] = stars[i].params['magV']\n",
    "    except: \n",
    "        star_dict[i]['magVisual'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['magL_nq_midinfared'] = stars[i].params['magL']\n",
    "    except:\n",
    "        star_dict[i]['magL_nq_midinfared'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['magM_midinfared'] = stars[i].params['magM']\n",
    "    except: \n",
    "        star_dict[i]['magM_midinfared'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['magN_midinfared'] = stars[i].params['magN']\n",
    "    except:\n",
    "        star_dict[i]['magN_midinfared'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['distance'] = stars[i].d\n",
    "    except:\n",
    "        star_dict[i]['distance'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['periastron'] = stars[i].params['periastron']\n",
    "    except:\n",
    "        star_dict[i]['periastron'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['right_ascension'] = stars[i].ra\n",
    "    except:\n",
    "        star_dict[i]['right_ascension'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['declination'] = stars[i].dec\n",
    "    except:\n",
    "        star_dict[i]['declination'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['parent_obj'] = getListOfNames(str(stars[i].parent))\n",
    "    except: \n",
    "        star_dict[i]['parent_obj'] = np.nan\n",
    "    try:\n",
    "        star_dict[i]['child_obj'] = getListOfNames(str(stars[i].children))\n",
    "    except: \n",
    "        star_dict[i]['child_obj'] = np.nan\n",
    "    try: \n",
    "        star_dict[i]['planet1type'] = stars[i].children[0].type()   \n",
    "    except: \n",
    "        star_dict[i]['planet1type'] = np.nan  \n",
    "    try: \n",
    "        star_dict[i]['planet2type'] = stars[i].children[1].type()   \n",
    "    except: \n",
    "        star_dict[i]['planet2type'] = np.nan \n",
    "    try: \n",
    "        star_dict[i]['planet3type'] = stars[i].children[2].type()   \n",
    "    except: \n",
    "        star_dict[i]['planet3type'] = np.nan \n",
    "    try: \n",
    "        star_dict[i]['planet4type'] = stars[i].children[3].type()   \n",
    "    except: \n",
    "        star_dict[i]['planet4type'] = np.nan \n",
    "    try: \n",
    "        star_dict[i]['planet5type'] = stars[i].children[4].type()   \n",
    "    except: \n",
    "        star_dict[i]['planet5type'] = np.nan \n",
    "    try: \n",
    "        star_dict[i]['planet6type'] = stars[i].children[5].type()   \n",
    "    except: \n",
    "        star_dict[i]['planet6type'] = np.nan \n",
    "    try: \n",
    "        star_dict[i]['planet7type'] = stars[i].children[6].type()   \n",
    "    except: \n",
    "        star_dict[i]['planet7type'] = np.nan \n",
    "    try: \n",
    "        star_dict[i]['planet8type'] = stars[i].children[7].type()   \n",
    "    except: \n",
    "        star_dict[i]['planet8type'] = np.nan \n",
    "    try: \n",
    "        star_dict[i]['planet9type'] = stars[i].children[8].type()   \n",
    "    except: \n",
    "        star_dict[i]['planet9type'] = np.nan \n",
    "    star_dict[i]['proper'] = stars[i].name\n",
    "    star_dict[i]['flags'] = stars[i].flags\n",
    "    star_dict[i]['system'] = getListOfNames(str(stars[i].system))\n",
    "    star_dict[i]['radius'] = stars[i].R\n",
    "    star_dict[i]['age'] = stars[i].age\n",
    "    star_dict[i]['hip'] = findOtherName(\"HIP \", star_dict[i]['proper'], star_dict[i]['altnames'])\n",
    "    star_dict[i]['hd'] = findOtherName(\"HD \", star_dict[i]['proper'], star_dict[i]['altnames'])\n",
    "    star_dict[i]['hr'] = findOtherName(\"HR \", star_dict[i]['proper'], star_dict[i]['altnames'])\n",
    "    star_dict[i]['gl'] = findGLName(star_dict[i]['proper'], star_dict[i]['altnames'])\n",
    "    i += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform all of my dictionaries to dataframes so I can work with them in pandas \n",
    "sdf = pd.DataFrame(star_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#realign them so my columns are on top and rows go downward \n",
    "stars = sdf.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine data\n",
    "Figure out what you might need to clean up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up stars to correctly cast to dtypes \n",
    "stars['radius'] = stars['radius'].str.strip(to_strip=\" R_s\")\n",
    "stars['age'] = stars['age'].str.strip(\" Gyr\")\n",
    "stars['mass'] = stars['mass'].str.strip(\" M_s\")\n",
    "stars['distance'] = stars['distance'].str.strip(\" pc\")\n",
    "stars['temp'] = stars['temp'].str.strip(\" K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with little to no information\n",
    "stars = stars.drop(['magL_nq_midinfared', 'magM_midinfared', 'magN_midinfared', 'periastron'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recast everything as dtypes we can work with \n",
    "stars['radius'] = stars['radius'].astype(float) \n",
    "stars['age'] = stars['age'].astype(float) \n",
    "stars['temp'] = stars['temp'].astype(float) \n",
    "stars['mass'] = stars['mass'].astype(float) \n",
    "stars['distance'] = stars['distance'].astype(float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of planets we have around each star \n",
    "child = stars['child_obj'].astype(str)\n",
    "childDict = {}\n",
    "for i in range(len(child)):\n",
    "    childDict[i] = {}\n",
    "    if child[i] == '':\n",
    "        childDict[i]['children'] = 0.0\n",
    "    else:\n",
    "        childDict[i]['children'] = float(child[i].count(',') + 1.0)\n",
    "childDict = pd.DataFrame.from_dict(childDict, orient='index', dtype=float)\n",
    "stars = childDict.merge(stars, right_index=True, left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to know how many total of each variety of planet each star has, to see if we can predict which type a star with a planet will have based on \n",
    "# its features. I have a maximum of 8 planets in any solar system (excluding pluto which is still on the list as a dwarf)\n",
    "columns = {'Cold Jupiter':'CJ', 'Cold Neptune':'CN', 'Cold Super-Earth':'CE', 'Hot Jupiter':'HE', \n",
    "           'Hot Neptune':'HN', 'Hot Super-Earth':'HSE', 'None Jupiter':'JUP', 'None Neptune':'NEP', \n",
    "           'None Super-Earth':'SE', 'Warm Jupiter':'WJ', 'Warm Neptune':'WN', 'Warm Super-Earth':'WSE'}\n",
    "\n",
    "dumms1 = pd.get_dummies(stars['planet1type'])\n",
    "dumms1 = dumms1.rename(columns=columns)\n",
    "dumms2 = pd.get_dummies(stars['planet2type'])\n",
    "dumms2 = dumms2.rename(columns=columns)\n",
    "dumms3 = pd.get_dummies(stars['planet3type'])\n",
    "dumms3 = dumms3.rename(columns=columns)\n",
    "dumms4 = pd.get_dummies(stars['planet4type'])\n",
    "dumms4 = dumms4.rename(columns=columns)\n",
    "dumms5 = pd.get_dummies(stars['planet5type'])\n",
    "dumms5 = dumms5.rename(columns=columns)\n",
    "dumms6 = pd.get_dummies(stars['planet6type'])\n",
    "dumms6 = dumms6.rename(columns=columns)\n",
    "dumms7 = pd.get_dummies(stars['planet7type'])\n",
    "dumms7 = dumms7.rename(columns=columns)\n",
    "dumms8 = pd.get_dummies(stars['planet8type'])\n",
    "dumms8 = dumms8.rename(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = dumms1.merge(dumms2, left_index=True, right_index=True, suffixes=('1', '2'))\n",
    "d2 = d1.merge(dumms3, left_index=True, right_index=True)\n",
    "d3 = d2.merge(dumms4, left_index=True, right_index=True, suffixes=('3', '4'))\n",
    "d4 = d3.merge(dumms5, left_index=True, right_index=True)\n",
    "d5 = d4.merge(dumms6, left_index=True, right_index=True, suffixes=('6', '6'))\n",
    "d6 = d5.merge(dumms7, left_index=True, right_index=True)\n",
    "pln_types = d6.merge(dumms8, left_index=True, right_index=True, suffixes=('7', '8'))\n",
    "pln_types = pln_types.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condense into catagories for each planet type \n",
    "pln_types['CJ'] = pln_types['CJ1'] + pln_types['CJ2'] + pln_types['CJ3'] + pln_types['CJ4'] + pln_types['CJ7'] + pln_types['CJ8'] \n",
    "pln_types['CN'] = pln_types['CN1'] + pln_types['CN2'] + pln_types['CN3'] + pln_types['CN4'] + pln_types['CN_x'] + pln_types['CN_y'] + pln_types['CN'] \n",
    "pln_types['CE'] = pln_types['CE1'] + pln_types['CE2'] + pln_types['CE3'] + pln_types['CE4'] + pln_types['CE'] \n",
    "pln_types['HE'] = pln_types['HE'] \n",
    "pln_types['HN'] = pln_types['HN1'] + pln_types['HN2'] \n",
    "pln_types['HSE'] = pln_types['HSE1'] + pln_types['HSE2'] + pln_types['HSE3'] \n",
    "pln_types['JUP'] = pln_types['JUP1'] + pln_types['JUP2'] + pln_types['JUP3']\n",
    "pln_types['NEP'] = pln_types['NEP1'] + pln_types['NEP2'] + pln_types['NEP']\n",
    "pln_types['SE'] = pln_types['SE1'] + pln_types['SE2'] + pln_types['SE3'] + pln_types['SE4']\n",
    "pln_types['WJ'] = pln_types['WJ1'] + pln_types['WJ2'] + pln_types['WJ3'] + pln_types['WJ4'] \n",
    "pln_types['WN'] = pln_types['WN1'] + pln_types['WN2'] + pln_types['WN3'] + pln_types['WN4']\n",
    "pln_types['WSE'] = pln_types['WSE1'] + pln_types['WSE2'] + pln_types['WSE3'] + pln_types['WSE4'] + pln_types['WSE7'] + pln_types['WSE8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of all of the excess planet types \n",
    "pln_types = pln_types.drop(columns=['CJ1', 'CN1', 'CE1', 'HN1', 'HSE1', 'JUP1', 'NEP1', 'SE1', 'WJ1', 'WN1', 'WSE1',\n",
    "                     'CJ2', 'CN2', 'CE2', 'HN2', 'HSE2', 'JUP2', 'NEP2', 'SE2', 'WJ2', 'WN2', 'WSE2', 'CJ3', \n",
    "                     'CN3', 'CE3', 'HSE3', 'JUP3', 'SE3', 'WJ3', 'WN3', 'WSE3', 'CJ4', 'CN4', 'CE4', \n",
    "                     'HSE4', 'JUP4', 'SE4', 'WJ4', 'WN4', 'WSE4', 'CJ6', 'CN_x', 'CE6', 'SE6', 'WSE6', 'CJ6',\n",
    "                     'CE6', 'SE6', 'WSE6', 'CJ7', 'CN_y', 'WSE7', 'CJ8', 'WSE8'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge your dummies frame with your main frame \n",
    "stars = stars.merge(pln_types, right_index=True, left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  make seperate frames for each of your name types, will try to merge on each of them \n",
    "HIP = stars.loc[stars['hip'] > 0]\n",
    "HD = stars.loc[stars['hd'] > 0]\n",
    "HR = stars.loc[stars['hr'] > 0]\n",
    "GL = stars.loc[stars['gl'].notnull()]\n",
    "proper = star_csv.loc[star_csv['proper'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge on Id for each name type, we don't have a match for every star so we only end up with 1,000~\n",
    "GL_csv = GL.merge(star_csv, on='gl')\n",
    "HD_csv = HD.merge(star_csv, on='hd')\n",
    "HR_csv = HR.merge(star_csv, on='hr')\n",
    "HIP_csv = HIP.merge(star_csv, on='hip')\n",
    "stars_csv = stars.merge(proper, on='proper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all of your frames back together & drop the duplicates (we know most stars have multiple ID names)\n",
    "HIP_csv = HIP_csv.append(GL_csv)\n",
    "HIP_csv = HIP_csv.append(HD_csv)\n",
    "HIP_csv = HIP_csv.append(stars_csv)\n",
    "HIP_csv = HIP_csv.append(HR_csv)\n",
    "HIP_csv = HIP_csv.drop_duplicates('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the readme that this data comes with says that it replaced null values with 1,000,000, we can try and fix that \n",
    "HIP_csv['dist'] = HIP_csv['dist'].replace(100000, np.nan)\n",
    "HIP_csv['dist'] = HIP_csv.fillna(HIP_csv['distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new estimates roughly 1 in 4 sunlike stars have planets, adjusting for how much data will be lost when dropping null values \n",
    "data = HIP_csv.append(star_csv.sample(3000, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>children</th>\n",
       "      <th>spectral_type</th>\n",
       "      <th>temp</th>\n",
       "      <th>metallicity</th>\n",
       "      <th>altnamesstr</th>\n",
       "      <th>altnames</th>\n",
       "      <th>mass</th>\n",
       "      <th>magUltraviolet</th>\n",
       "      <th>magBlue</th>\n",
       "      <th>magH_nearinfared</th>\n",
       "      <th>...</th>\n",
       "      <th>lum</th>\n",
       "      <th>var</th>\n",
       "      <th>var_min</th>\n",
       "      <th>var_max</th>\n",
       "      <th>hip_x</th>\n",
       "      <th>gl</th>\n",
       "      <th>hip_y</th>\n",
       "      <th>hd</th>\n",
       "      <th>proper</th>\n",
       "      <th>hr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>G8 III</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>11 Comae Berenices,HD 107383,HIP 60202,TYC 144...</td>\n",
       "      <td>[11 Comae Berenices, HD 107383, HIP 60202, TYC...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.74</td>\n",
       "      <td>2.484</td>\n",
       "      <td>...</td>\n",
       "      <td>147.638628</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>K4III</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.04</td>\n",
       "      <td>11 Ursae Minoris,Pherkard,Pherkad Minor,HD 136...</td>\n",
       "      <td>[11 Ursae Minoris, Pherkard, Pherkad Minor, HD...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.415</td>\n",
       "      <td>2.091</td>\n",
       "      <td>...</td>\n",
       "      <td>16.173351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>K0III</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>14 Andromedae,HD 221345,HIP 116076,TYC 3231-32...</td>\n",
       "      <td>[14 Andromedae, HD 221345, HIP 116076, TYC 323...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.24</td>\n",
       "      <td>2.608</td>\n",
       "      <td>...</td>\n",
       "      <td>1.858660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.000</td>\n",
       "      <td>10.850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>K0 V</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.43</td>\n",
       "      <td>HD 145675,HIP 79248,TYC 3067-576-1,SAO 45933,G...</td>\n",
       "      <td>[HD 145675, HIP 79248, TYC 3067-576-1, SAO 459...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.57</td>\n",
       "      <td>4.803</td>\n",
       "      <td>...</td>\n",
       "      <td>218.373533</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>G2V</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.096</td>\n",
       "      <td>16 Cyg A,HD 186408,HIP 96895,TYC 3565-1524-1,S...</td>\n",
       "      <td>[16 Cyg A, HD 186408, HIP 96895, TYC 3565-1524...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.59</td>\n",
       "      <td>4.72</td>\n",
       "      <td>...</td>\n",
       "      <td>1.320687</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.502</td>\n",
       "      <td>10.392</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117519</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>162.031704</td>\n",
       "      <td>XZ</td>\n",
       "      <td>5.851</td>\n",
       "      <td>5.651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>224062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66049</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>30.060763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118204</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37428</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8.120822</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61522</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39674</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>285759.054337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.268</td>\n",
       "      <td>11.168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54328</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>91.201084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3979 rows Ã— 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        children spectral_type  temp metallicity  \\\n",
       "0            1.0        G8 III   NaN       -0.35   \n",
       "1            1.0         K4III   NaN        0.04   \n",
       "2            1.0         K0III   NaN       -0.24   \n",
       "3            2.0          K0 V   NaN        0.43   \n",
       "4            0.0           G2V   NaN       0.096   \n",
       "...          ...           ...   ...         ...   \n",
       "117519       NaN           NaN   NaN         NaN   \n",
       "66049        NaN           NaN   NaN         NaN   \n",
       "37428        NaN           NaN   NaN         NaN   \n",
       "39674        NaN           NaN   NaN         NaN   \n",
       "54328        NaN           NaN   NaN         NaN   \n",
       "\n",
       "                                              altnamesstr  \\\n",
       "0       11 Comae Berenices,HD 107383,HIP 60202,TYC 144...   \n",
       "1       11 Ursae Minoris,Pherkard,Pherkad Minor,HD 136...   \n",
       "2       14 Andromedae,HD 221345,HIP 116076,TYC 3231-32...   \n",
       "3       HD 145675,HIP 79248,TYC 3067-576-1,SAO 45933,G...   \n",
       "4       16 Cyg A,HD 186408,HIP 96895,TYC 3565-1524-1,S...   \n",
       "...                                                   ...   \n",
       "117519                                                NaN   \n",
       "66049                                                 NaN   \n",
       "37428                                                 NaN   \n",
       "39674                                                 NaN   \n",
       "54328                                                 NaN   \n",
       "\n",
       "                                                 altnames  mass  \\\n",
       "0       [11 Comae Berenices, HD 107383, HIP 60202, TYC...   NaN   \n",
       "1       [11 Ursae Minoris, Pherkard, Pherkad Minor, HD...   NaN   \n",
       "2       [14 Andromedae, HD 221345, HIP 116076, TYC 323...   NaN   \n",
       "3       [HD 145675, HIP 79248, TYC 3067-576-1, SAO 459...   NaN   \n",
       "4       [16 Cyg A, HD 186408, HIP 96895, TYC 3565-1524...   NaN   \n",
       "...                                                   ...   ...   \n",
       "117519                                                NaN   NaN   \n",
       "66049                                                 NaN   NaN   \n",
       "37428                                                 NaN   NaN   \n",
       "39674                                                 NaN   NaN   \n",
       "54328                                                 NaN   NaN   \n",
       "\n",
       "       magUltraviolet magBlue magH_nearinfared  ...            lum  var  \\\n",
       "0                 NaN    5.74            2.484  ...     147.638628  NaN   \n",
       "1                 NaN   6.415            2.091  ...      16.173351  NaN   \n",
       "2                 NaN    6.24            2.608  ...       1.858660  NaN   \n",
       "3                 NaN    7.57            4.803  ...     218.373533  NaN   \n",
       "4                 NaN    6.59             4.72  ...       1.320687  NaN   \n",
       "...               ...     ...              ...  ...            ...  ...   \n",
       "117519            NaN     NaN              NaN  ...     162.031704   XZ   \n",
       "66049             NaN     NaN              NaN  ...      30.060763  NaN   \n",
       "37428             NaN     NaN              NaN  ...       8.120822  NaN   \n",
       "39674             NaN     NaN              NaN  ...  285759.054337  NaN   \n",
       "54328             NaN     NaN              NaN  ...      91.201084  NaN   \n",
       "\n",
       "       var_min var_max  hip_x   gl hip_y      hd proper    hr  \n",
       "0          NaN     NaN    NaN  NaN   NaN     NaN    NaN   NaN  \n",
       "1          NaN     NaN    NaN  NaN   NaN     NaN    NaN   NaN  \n",
       "2       11.000  10.850    NaN  NaN   NaN     NaN    NaN   NaN  \n",
       "3          NaN     NaN    NaN  NaN   NaN     NaN    NaN   NaN  \n",
       "4       10.502  10.392    NaN  NaN   NaN     NaN    NaN   NaN  \n",
       "...        ...     ...    ...  ...   ...     ...    ...   ...  \n",
       "117519   5.851   5.651    NaN  NaN   NaN  224062    NaN  9047  \n",
       "66049      NaN     NaN    NaN  NaN   NaN  118204    NaN   NaN  \n",
       "37428      NaN     NaN    NaN  NaN   NaN   61522    NaN   NaN  \n",
       "39674   11.268  11.168    NaN  NaN   NaN     NaN    NaN   NaN  \n",
       "54328      NaN     NaN    NaN  NaN   NaN   96738    NaN  4332  \n",
       "\n",
       "[3979 rows x 92 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we've added more data that didn't have these columns they'll all by NaN \n",
    "list_ = ['HE', 'NEP', 'CE', 'CN', 'CJ', 'HN', 'HSE', 'JUP', 'SE', 'WJ', 'WN', 'WSE', 'children']\n",
    "for item in list_:\n",
    "    data[item] = data[item].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns & 'NaN' values, if you drop too many, just sample more random data & try and keep it \n",
    "# relitively proportionate \n",
    "data = data[['children', 'ra', 'dec', 'dist', 'pmra', 'pmdec', 'rv', 'mag', 'absmag', 'spect', 'ci', 'x', 'y', 'z', 'vx', 'vy', 'vz', 'rarad', \n",
    "             'decrad', 'pmrarad', 'pmdecrad','comp_primary', 'lum','HE', 'NEP', 'CE', 'CN', 'CJ', 'HN', 'HSE', 'JUP', 'SE', 'WJ', 'WN', 'WSE']].copy()\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectral type is another catagorical variable that we've got to do something about, unfortunatley \n",
    "# theyre a mess and written too inconsistently to easily make dummies of  \n",
    "data['temp_class'] = data['spect'].str[:1]\n",
    "data['temp_class'] = data['temp_class'].fillna('nan')\n",
    "\n",
    "data['heat_class'] = data['spect'].str[1:2]\n",
    "data['heat_class'] = data['heat_class'].fillna('nan')\n",
    "\n",
    "data['lum_class'] = data['spect'].str[2:]\n",
    "data['lum_class'] = data['lum_class'].fillna('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dummies adds a few hundred columns, most of which with only one star in it\n",
    "data = data.reset_index()\n",
    "data = pd.get_dummies(data, columns=['temp_class'], prefix='_y')\n",
    "data = pd.get_dummies(data, columns=['heat_class'], prefix='_x')\n",
    "data = pd.get_dummies(data, columns=['lum_class'], prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fix that, we'll be grouping all of our stars together by spectral type & splitting them up a bit more \n",
    "to_lum = [\n",
    "            ['I' , \n",
    "                 ['_.5Ib', '_/B8Ib', '_Ia', '_Ib', '_Ib/II', '_.5Iab:ne', '_Iab:var', '_-G2Ie', '_Ib-II', '_Ia0:', '_Iab:', \n",
    "                  '_Ib+...', '_-F8Ib', '_Ia/ab', '_Iab']],     \n",
    "            ['II',\n",
    "                 ['_Ib/II', '_Ib-II', '_/B9II/III', '_/K0II', '_II', '_II-III', '_II/III', '_IICNp...', '_IIb', '_IIp...',\n",
    "                     '_II/IIICN', '_IICN...', '_/2II/III+A', '_/F2III', '_II + A/F', '_II+...']], \n",
    "            ['III', \n",
    "                 ['_.5III', '_.5III:', '_/G5III', '_/G8III', '_/B8III', '_/B9II/III', '_/B9III/IV', '_/A8III', '_/G8III:', \n",
    "                  '_/K0III', '_/K0III:', '_/K1III', '_/K1III+..', '_/K2III', '_/K3III', '_/K3III:', '_/K4III', '_/K4III:', \n",
    "                  '_/M0III', '_/M1III', '_/M3III', '_:III:', '_III+...', '_II-III', '_/K5III', '_II/IIICNV:', '_III:', \n",
    "                  '_/M2III', '_/2II/III+A', '_/K2III:', '_/M1III:', '_III comp', '_/F5III', '_II/IIICN', '_III', '_IIICN...',\n",
    "                  '_III + (F)', '_III-IV', '_III...', '_III/IV', '_IIICNII', '_IIICNp...', '_III-IV SB', '_III/IVCN.', \n",
    "                  '_/G6III', '_/K0III+..', '_/K0IIICNp', '_IIIb', '_IIIsp...', '_IIIp', '_IIIvar', '_.5III/IV']], \n",
    "            ['IV', \n",
    "                 ['_.5IV', '_/B9III/IV', '_/A3IV', '_/A3IV/V', '_/A7IV', '_/F2IV', '_/F2IV/V', '_/F7IV/V', '_III/IV', '_/K0IV/V', \n",
    "                  '_IV', '_IV:pe...', '_IVne+...', '_/F6IV/V', '_/F8IV+...', '_/G8IV/V', '_:IVp', '_IVCN...', '_.5III/IV', \n",
    "                  '_IVn', '_.5IV-V', '_/A4IV', '_IV-V', '_/K1IV', '_III-IV', '_IV/V', '_/K1IV/V:', '_IV...', '_III/IVCN.',\n",
    "                  '_IV: (+F/G)', '_/F3IV', '_/B3IV', '_/F3IV/V', '_/F5IV/V', '_/F5IV', '_III-IV SB']],\n",
    "            ['V', \n",
    "                 ['_ V', '_.5V', '_.5Ve', '_.5Vn', '_/A1V', '_/A2V', '_/A3IV/V', '_Vm', '_/K1IV/V:', '_/A3V', '_/A3V+...', '_V+...', \n",
    "                  '_/B3V', '_/F2IV/V', '_/G0V', '_/K0IV/V', '_V + G/K', '_Ve', '_Vvar', '_/K3V:+...', '_V comp SB', '_Vpe', '_Vws', \n",
    "                  '_/F3V', '_/G1V', '_/K0V',  '_Ve+...', '_/B5V', '_/F5V', '_/G2V', '_/K1V', '_V-VI', '_Vn', '_/F2V', '_/F6V', \n",
    "                  '_/G3V', '_V...', '_Vne', '_/F3IV/V', '_/K3V', '_/B8V', '_/F7IV/V', '_/G5V', '_/M2V', '_V:', '_Vp', '_/F5IV/V', \n",
    "                  '_/B9V', '_Vp...', '_/F7V', '_/F8V', '_/G6V', '_/G8V', '_/B9.5V', '_:V...', '_IV-V', '_V', '_V:n', '_V:pe',\n",
    "                  '_Vw...', '_/F6IV/V', '_/G8IV/V', '_V comp', '_VCN...', '_.5IV-V']],\n",
    "            ['VI', \n",
    "                 ['_V-VI']]]\n",
    "\n",
    "to_heat = [\n",
    "            ['0', \n",
    "                 ['_x_0', '_/G0V', '_/G0Vs...', '_/K0II', '_/K0III', '_/K0III:', '_/K0IV/V', '_/K0V', '_/K0V + A/F', '_/K0p...', '_/M0III', \n",
    "                  '_0', '_Ia0:', '_/K0III+..', '_/K0IIICNp']],\n",
    "            ['1', \n",
    "                 ['_x_1', '_/A1V', '_/G1V', '_/G2V', '_/K1III', '_/K1III+..', '_/K1IV', '_/M1III', '_/K1IV/V:', '_/M1III:']],\n",
    "            ['2', \n",
    "                 ['_x_2', '_-G2Ie', '_/A2V', '_/F2IV', '_/F2IV/V', '_/K2III', '_/M2V', '_G2', '_/M2III', '_/2II/III+A', '_/F2III', '_/F2V',\n",
    "                  '_/K2III:']],\n",
    "            ['3', \n",
    "                 ['_x_3', '_/A3III', '_/A3IV', '_/A3IV/V', '_/A3V', '_/A3V+...', '_/B3V', '_/F3', '_/F3V', '_/G2V', '_/K3III', '_/K3III:', \n",
    "                  '_/K3V', '_/M3III', '_3', '_3   :', '_/F3IV', '_/B3IV', '_/F3IV/V', '_/K3V:+...']],\n",
    "            ['4', \n",
    "                 ['_x_4', '_/K4', '_/K4III', '_/K4III:', '_/A4IV']],\n",
    "            ['5', \n",
    "                 ['_x_5', '_/F5V', '_/G5III', '_/G5V', '_/G5Vw...', '_5', '_/F5IV', '_/K5III', '_/B5V', '_/F5III', '_/F5IV/V']],\n",
    "            ['6', \n",
    "                 ['_x_6', '_/F6V', '_/G6V', '_6', '_6 (SB1)', '_/F6IV/V', '_/G6III']],\n",
    "            ['7', \n",
    "                 ['_x_7', '_/A7IV', '_/F7IV/V', '_/F7V', '_7', '_e-M7e']],\n",
    "            ['8', \n",
    "                 ['_x_8', '_/B8III', '_/B8Ib', '_/B8V', '_/F8V', '_/G8III', '_/G8III:', '_/F8IV+...', '_/G8V', '_/G8w...', '_/O8', '_8', \n",
    "                  '_e-M8e', '_/A8III', '_-F8Ib', '_/G8IV/V']],\n",
    "            ['9', \n",
    "                 ['_x_9', '_/B9II/III', '_/B9III/IV', '_/B9V', '_9', '_9?', '_e-M9e', '_/B9.5V']]]\n",
    "\n",
    "to_temp = [\n",
    "            ['A', \n",
    "                 ['_y_A', '_x_A', '_/A1V', '_/A2V', '_/A3III', '_/A3IV', '_/A3IV/V', '_/A3V', '_/A7IV', '_/A3V+...', '_/A8III', '_/A4IV', '_/2II/III+A'\n",
    "                 ]],\n",
    "            ['B', \n",
    "                 ['_/B3V', '_/B8III', '_/B8Ib', '_/B8V', '_/B9II/III', '_/B9III/IV', '_/B9V', '_y_B', '_/B3IV','_/B9.5V', '_/B5V']],\n",
    "            ['C', \n",
    "                 ['_y_C', '_x_C']],\n",
    "            ['F', \n",
    "                 ['_/F2IV', '_/F2IV/V', '_/F3', '_/F3V', '_/F5V', '_/F6V', '_/F6IV/V', '_/F8IV+...', '_/F7IV/V', '_/F7V', '_/F8V', '_y_F',\n",
    "                  '_/F3IV', '_/F5IV', '_-F8Ib', '_/F2III', '_/F2V', '_/F3IV/V', '_/F5IV/V', '_/F2V', '_/F3IV/V',\n",
    "                  '_/F5III', '_/F5IV/V', '_/F6IV/V', '_/F8IV+...', '_F:']],\n",
    "            ['G', \n",
    "                 ['_y_G', '_-G2Ie', '_/G0V', '_/G1V', '_/G2V', '_/G3V', '_/G5III', '_/G5V', '_/G5Vw...', '_/G6V', '_/G8III', '_/G8V',\n",
    "                  '_/G8w...', '_G2', '_/G6III', '_/G8IV/V']],\n",
    "            ['K', \n",
    "                 ['_y_K', '_x_K', '_/K0II', '_/K0III', '_/K0III:', '_/K0IV/V', '_/K0V', '_/K0V + A/F', '_/K0p...', '_/K1III', '_/K1III+..', \n",
    "                  '_/K1IV', '_/K1V', '_/K2III', '_/K1IV/V:', '_/K3III', '_/K3III:', '_/K3V', '_/K4', '_/K4III', '_/K4III:', '_y_k', '_/K5III',\n",
    "                  '_/K0IIICNp', '_/K2III:', '_/K3V:+...', '_/K0III+..']],\n",
    "            ['N', \n",
    "                 ['_y_N', '_x_N']],\n",
    "            ['M', \n",
    "                 ['_y_M', '_x_M', '_/M0III', '_/M1III', '_/M2V', '_/M3III', '_e-M8e', '_e-M9e', '_/M2III', '_e-M7e', '_/M1III:']],\n",
    "            ['m',\n",
    "                 ['_m', '_m...', '_mp', '_Vm', '_x_m']],\n",
    "            ['RD', \n",
    "                 ['_y_R']],\n",
    "            ['sd', \n",
    "                 ['_y_s', '_x_d', '_V-VI']],\n",
    "            ['W', \n",
    "                 ['_y_W']],\n",
    "            ['O', \n",
    "                 ['_y_O', '_/O8', '_O:', '_Ia0:']],\n",
    "            ['D', \n",
    "                 ['_y_D']],\n",
    "            ['p', \n",
    "                ['_V:pe', '_Vp', '_Vp...', '_Vpe', '_p', '_p...', '_psh', '_sp...', '_:IVp', '_IIIp', '_IIIp...', '_IIICNp...', '_IICNp...', '_/K0p...',\n",
    "                 '_/K0IIICNp']],\n",
    "            ['n', \n",
    "                 ['_.5Vn', '_IVn', '_V:n', '_Vn', '_Vne', '_x_n', '_IVne+...', '_npe', '_.5Iab:ne']],\n",
    "            ['e', \n",
    "                 ['_IV:pe...', '_V:pe', '_Ve', '_Ve+...', '_Vne', '_e', '_e-M7e', '_.5Iab:ne', '_e-M8e', '_e-M9e', '_x_e', '_.5e', '_IVne+...', '_Vpe', \n",
    "                  '_-G2Ie', '_.5Ve', '_ev']]] \n",
    "to_symb = [['...', \n",
    "                 ['_+...', '_..', '_...', '_w...', '_sp...', '_V...', '_p...', '_m...', '_Vw...', '_Ve+...', '_IV:pe...', '_III...',\n",
    "                  '_IICNp...', '_:w...', '_III+...', '_/K0p...', '_/G0Vs...', '_IVne+...', '_IIIsp...', '_Ib+...', '_:V...',\n",
    "                  '_:III:+...', '_:+...', '_/K1III+..', '_/G5Vw...', '_/A3V+...', '_/G8w...', '_IIICN...', '_IIIp...', '_/F8IV+...', \n",
    "                  '_V+...', '_VCN...', '_IIp...', '_Vp...', '_IICN...', '_IV...', '_/K0III+..', '_/K3V:+...', '_:Vw...', '_II+...', '_IVCN...']],\n",
    "          [':', \n",
    "                 ['_.5III:', '_/G8III:', '_/K0III:', '_:+...', '_:III:+...', '_:w...', '_:V...', '_/K3III:', '_/K4III:', '_:',\n",
    "                  '_II/IIICNV:', '_III:', '_:III:', '_Ia0:', '_IV: (+F/G)', '_IV:pe...', '_O:', '_V:', '_V:n', '_V:pe', '_/K1IV/V:', '_Iab:', \n",
    "                  '_.5Iab:ne', '_/K2III:', '_/K3V:+...', '_/M1III:', '_:IVp', '_:Vw...', '_F:']],\n",
    "          ['+', \n",
    "                 ['_+...', '_/A3V+...', '_/K0V + A/F', '_Ve+...', '_:III:+...', '_/2II/III+A', '_III+...', '_:+...', '_/K1III+..', '_V + G/K', \n",
    "                  '_/K3V:+...', '_II + A/F', '_V+...', '_IVne+...', '_Ib+...', '_/F8IV+...', '_/K0III+..', '_II+...']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning our big ol lists into combined columns\n",
    "to_cat = [to_temp, to_heat, to_lum, to_symb]\n",
    "\n",
    "for item in to_cat:\n",
    "    for i in range(len(item)):\n",
    "                   catagorize(item[i][1], item[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only the columns we need is easier than trying to figure out which few hundred to drop \n",
    "data = data[['children', 'ra', 'dec', 'dist', 'pmra', 'pmdec', 'rv', 'mag', 'absmag', \n",
    "             'ci', 'x', 'y', 'z', 'vx', 'vy', 'vz', 'rarad', 'decrad', 'pmrarad', 'pmdecrad',\n",
    "             'comp_primary', 'lum', 'HE', 'NEP', 'CE', 'CN', 'CJ', 'HN', 'HSE', 'JUP', 'SE',\n",
    "             'WJ', 'WN', 'WSE', 'I', 'II', 'III', 'IV', 'V', 'VI', '...', ':', '+', '0', '1',\n",
    "             '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'F', 'G', 'K', 'N',\n",
    "             'M', 'RD', 'sd', 'W', 'O', 'D', 'n', 'e', 'p', 'm']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert everything to floats to get it ready to model \n",
    "data['ra'] = data['ra'].astype(float)\n",
    "data['dec'] = data['dec'].astype(float)\n",
    "data['dist'] = data['dist'].astype(float)\n",
    "data['pmra'] = data['pmra'].astype(float)\n",
    "data['pmdec'] = data['pmdec'].astype(float)\n",
    "data['rv'] = data['rv'].astype(float)\n",
    "data['mag'] = data['mag'].astype(float)\n",
    "data['absmag'] = data['absmag'].astype(float)\n",
    "data['x'] = data['x'].astype(float)\n",
    "data['y'] = data['y'].astype(float)\n",
    "data['z'] = data['z'].astype(float)\n",
    "data['vx'] = data['vx'].astype(float)\n",
    "data['vz'] = data['vz'].astype(float)\n",
    "data['vy'] = data['vy'].astype(float)\n",
    "data['rarad'] = data['rarad'].astype(float)\n",
    "data['decrad'] = data['decrad'].astype(float)\n",
    "data['pmrarad'] = data['pmrarad'].astype(float)\n",
    "data['pmdecrad'] = data['pmdecrad'].astype(float)\n",
    "data['comp_primary'] = data['comp_primary'].astype(float)\n",
    "data['lum'] = data['lum'].astype(float)\n",
    "data['ci'] = data['ci'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export so we don't have to keep re-running these cells! \n",
    "data.to_csv('scrubbed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
